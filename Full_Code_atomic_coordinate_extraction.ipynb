{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSpPhAsN-GIT"
      },
      "source": [
        "# Data Cleaning Procedure:\n",
        "\n",
        "We are working with a dataset containing protein names from the PDB site, and our goal is to streamline the data cleaning process using automated methods. While manual downloading is possible on the site, we aim to create a code that can handle this task efficiently.\n",
        "\n",
        "Here's the step-by-step process we'll follow:\n",
        "\n",
        "1. **Data Retrieval and Initial Filtering:**\n",
        "   We will initiate the process by extracting protein names from the PDB site. Our code will then automatically download the corresponding files. This approach simplifies data scraping and enhances efficiency, providing a structured dataset. While direct manual downloads are feasible, automation offers more streamlined handling.\n",
        "\n",
        "2. **Categorization of Protein Types:**\n",
        "   After the initial download, we will categorize the proteins based on their types. Proteins that are DNAs (NMR) and EM (complexes) will be identified and segregated into a separate CSV file. This segregation ensures that the dataset remains organized and suitable for downstream analyses.\n",
        "\n",
        "3. **Refining the Dataset:**\n",
        "   We will proceed by eliminating undesired data points. This involves removing proteins with low resolutions and excluding those that are not based on X-ray experiment types. This refinement step ensures that the dataset consists of high-quality, relevant protein structures for further processing.\n",
        "\n",
        "4. **Handling Multi-Chain Files:**\n",
        "   For proteins composed of multiple chains, such as the example 1abc with 2 chains, we will create distinct entries for each chain. This separation results in entries like 1abc_1 and 1abc_2. By doing this, we maintain granularity in our dataset, enabling more precise analyses.\n",
        "\n",
        "\n",
        "By following this refined data cleaning procedure, we aim to establish a well-structured and comprehensive dataset that can be effectively utilized for various analyses and modeling tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxsAyerlcps8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install biopython\n",
        "!pip install biopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlLmc40Qcroy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "import requests\n",
        "import numpy as np\n",
        "from Bio import PDB\n",
        "from Bio.PDB import PDBParser\n",
        "import tensorflow as tf\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fKzeURtZjkw"
      },
      "outputs": [],
      "source": [
        "#Name of the proteins File\n",
        "csv_path = \"/content/drive/MyDrive/pdb_entry_files.csv\"\n",
        "df = pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpfnOBKRZmHf"
      },
      "source": [
        "#first delete nucliotides only, those who are NMR, and seperate the EM complexes in another file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2p87VBcZkpE"
      },
      "outputs": [],
      "source": [
        "def save_and_remove_data(dataframe, csv_path):\n",
        "    # Remove rows with 'nuc' in the second column\n",
        "    dataframe = dataframe[dataframe.iloc[:, 1] != 'nuc']\n",
        "\n",
        "    # Remove rows with 'NMR' in the third column\n",
        "    dataframe = dataframe[dataframe.iloc[:, 2] != 'NMR']\n",
        "\n",
        "    # Save the removed rows with 'EM' in the third column to a new CSV file\n",
        "    em_rows = dataframe[dataframe.iloc[:, 2] == 'EM']\n",
        "    em_rows.to_csv('em_rows.csv', index=False)\n",
        "\n",
        "    # Remove rows with 'EM' in the third column\n",
        "    dataframe = dataframe[dataframe.iloc[:, 2] != 'EM']\n",
        "\n",
        "    # Save the rows with 'prot-nuc' in the second column to a new CSV file\n",
        "    prot_nuc_rows = dataframe[dataframe.iloc[:, 1] == 'prot-nuc']\n",
        "    prot_nuc_rows.to_csv('prot_nuc_rows.csv', index=False)\n",
        "\n",
        "    # Remove rows with 'prot-nuc' in the second column\n",
        "    dataframe = dataframe[dataframe.iloc[:, 1] != 'prot-nuc']\n",
        "\n",
        "    # Save the modified DataFrame to a new CSV file\n",
        "    dataframe.to_csv(csv_path, index=False)\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "# Read the original CSV file into a DataFrame\n",
        "original_csv_path = \"/content/drive/MyDrive/pdb_entry_files.csv\"\n",
        "df = pd.read_csv(original_csv_path)\n",
        "\n",
        "# Call the function to save EM rows, 'prot-nuc' rows, remove 'nuc', 'NMR', 'EM', and 'prot-nuc' rows\n",
        "modified_df = save_and_remove_data(df, 'modified_data.csv')\n",
        "\n",
        "# Now, the 'modified_df' DataFrame contains the desired modifications and is saved in 'modified_data.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikzDkutwZrrS"
      },
      "outputs": [],
      "source": [
        "#dont forget to remove the first row in all the dataset\n",
        "prot_EM = '/content/em_rows.csv'\n",
        "prot_nuc = '/content/prot_nuc_rows.csv'\n",
        "prot_csv = '/content/modified_data.csv'\n",
        "\n",
        "df = pd.read_csv(prot_csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXHV-po7ZsQ8"
      },
      "source": [
        "#note:\n",
        "the number of proteins are nearly 167k.\n",
        "Now, this is what we should download our pdb files off of.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqktsysEZ1pX"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def create_prot_name_list(csv_path):\n",
        "    prot_list = []\n",
        "    with open(csv_path, 'r') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        for row in csv_reader:\n",
        "            prot_list.append(row[0])\n",
        "    return prot_list\n",
        "\n",
        "#create a list to download PDB files based on the names\n",
        "protein_names = create_prot_name_list(prot_csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxxkud35Z2-7"
      },
      "source": [
        "#download the pdb files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IenHuLWnZ6JX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "class PDBDownloader:\n",
        "    def __init__(self, destination_path):\n",
        "        self.destination_path = destination_path\n",
        "\n",
        "    def download_pdb(self, protein_name_list):\n",
        "        for protein_name in protein_name_list:\n",
        "            filename = f\"{protein_name}.pdb\"\n",
        "            file_path = os.path.join(self.destination_path, filename)\n",
        "\n",
        "            # Check if the file already exists in the destination path\n",
        "            if os.path.exists(file_path):\n",
        "                print(f\"File already exists for {protein_name}, skipping download.\")\n",
        "                continue\n",
        "\n",
        "            url = f\"https://files.rcsb.org/download/{protein_name}.pdb\"\n",
        "\n",
        "            try:\n",
        "                response = requests.get(url)\n",
        "                response.raise_for_status()\n",
        "                pdb_content = response.text\n",
        "\n",
        "                # Check if the response contains the PDB content\n",
        "                if \"HEADER    \" not in pdb_content:\n",
        "                    print(f\"No PDB file found for {protein_name}\")\n",
        "                    continue\n",
        "\n",
        "                with open(file_path, \"w\") as file:\n",
        "                    file.write(pdb_content)\n",
        "\n",
        "                print(f\"Downloaded PDB file for {protein_name}\")\n",
        "            except requests.HTTPError as e:\n",
        "                print(f\"Failed to download PDB file for {protein_name}\")\n",
        "                print(f\"HTTP Error: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to download PDB file for {protein_name}\")\n",
        "                print(f\"Error: {e}\")\n",
        "\n",
        "# Example usage\n",
        "downloader = PDBDownloader(destination_path=\"/content/drive/MyDrive/NewPDBFiles\")\n",
        "protein_names = protein_names\n",
        "downloader.download_pdb(protein_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KLYWBhqIoyE"
      },
      "source": [
        "# Removing Low Resolutions, Non-X-Ray Entries, and DNA Files:\n",
        "We will exclude proteins with low resolutions and those that are not based on X-ray experiments. Additionally, we will omit DNA files from the dataset. This step is essential as the shared letters in DNA sequences might create confusion for the model, even though theoretically the model could potentially distinguish them. However, the intricacies of this theoretical capability are beyond the scope of our current discussion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE6b4tAdIzJz"
      },
      "outputs": [],
      "source": [
        "#RESOLUTION\n",
        "def extract_resolution(pdb_file_path):\n",
        "    with open(pdb_file_path, 'r') as pdb_file:\n",
        "        for line in pdb_file:\n",
        "            match = re.match(r'REMARK\\s+2\\s+RESOLUTION\\.\\s+(\\d+\\.\\d+)\\s+ANGSTROMS\\.', line)\n",
        "            if match:\n",
        "                resolution_str = match.group(1)\n",
        "                try:\n",
        "                    resolution = float(resolution_str)\n",
        "                    return resolution\n",
        "                except ValueError:\n",
        "                    print(\"Error: Resolution information is not in a valid numerical format.\")\n",
        "                    return None\n",
        "\n",
        "    print(f\"Resolution information not found in {pdb_file_path}.\")\n",
        "    return None\n",
        "\n",
        "#EXPERIMENT TYPE\n",
        "def extract_experiment_type(pdb_file_path):\n",
        "    with open(pdb_file_path, 'r') as pdb_file:\n",
        "        for line in pdb_file:\n",
        "            if line.startswith('REMARK 200  EXPERIMENT TYPE'):\n",
        "                experiment_type = line.split(':')[1].strip()\n",
        "                return experiment_type\n",
        "\n",
        "    print(\"Experiment type information not found.\")\n",
        "    return None\n",
        "\n",
        "#DNA FILES\n",
        "def remove_pdb_files_with_header(folder_path):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.pdb'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, 'r') as pdb_file:\n",
        "                lines = pdb_file.readlines()\n",
        "                if any(line.startswith('HEADER    DNA') for line in lines):\n",
        "                    print(f\"Removing file: {filename}\")\n",
        "                    os.remove(file_path)\n",
        "\n",
        "def delete_files_not_meeting_criteria(directory_path, threshold_resolution, target_experiment_type):\n",
        "    num_deleted_files = 0  # Initialize the counter for deleted files\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".pdb\"):\n",
        "            pdb_file_path = os.path.join(directory_path, filename)\n",
        "            resolution = extract_resolution(pdb_file_path)\n",
        "            experiment_type = extract_experiment_type(pdb_file_path)\n",
        "\n",
        "            if resolution is not None and experiment_type is not None:\n",
        "                # Check if the file meets any of the criteria\n",
        "                if resolution < threshold_resolution or experiment_type != target_experiment_type:\n",
        "                    print(f\"Deleting file: {pdb_file_path} with resolution {resolution:.2f} Ã… and experiment type {experiment_type}\")\n",
        "                    os.remove(pdb_file_path)\n",
        "                    num_deleted_files += 1  # Increment the counter for deleted files\n",
        "                else:\n",
        "                    # If the file passes the resolution and experiment type criteria,\n",
        "                    # check if it also has the 'HEADER DNA' line, and remove it.\n",
        "                    with open(pdb_file_path, 'r') as pdb_file:\n",
        "                        lines = pdb_file.readlines()\n",
        "                    if any(line.startswith('HEADER    DNA') for line in lines):\n",
        "                        print(f\"Removing file: {pdb_file_path}\")\n",
        "                        os.remove(pdb_file_path)\n",
        "                        num_deleted_files += 1  # Increment the counter for deleted files\n",
        "\n",
        "    print(f\"Total files deleted: {num_deleted_files}\")\n",
        "\n",
        "# Replace 'path/to/your/directory' with the actual directory path containing the PDB files.\n",
        "directory_path = \"/content/drive/MyDrive/NewPDBFiles/\"\n",
        "threshold_resolution = 2.0  # Set the threshold resolution below which files will be deleted.\n",
        "target_experiment_type = \"X-ray diffraction\"  # Set the desired experiment type.\n",
        "\n",
        "delete_files_not_meeting_criteria(directory_path, threshold_resolution, target_experiment_type)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7MtgYIw2iaq"
      },
      "source": [
        "#Now the clean up happened, we have to separate chains and have them in a new folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2IrYfChN2gz3"
      },
      "outputs": [],
      "source": [
        "def split_pdb_chains(input_pdb_path, output_folder):\n",
        "    parser = PDB.PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure(\"protein\", input_pdb_path)\n",
        "\n",
        "    for model in structure:\n",
        "        chains = list(model)\n",
        "\n",
        "        for chain in chains:\n",
        "            chain_id = chain.id\n",
        "            base_filename = os.path.splitext(os.path.basename(input_pdb_path))[0]\n",
        "            chain_output_path = os.path.join(output_folder, f\"{base_filename}_{chain_id}.pdb\")\n",
        "\n",
        "            io = PDB.PDBIO()\n",
        "            io.set_structure(chain)\n",
        "            io.save(chain_output_path)\n",
        "\n",
        "            if len(chains) == 1:\n",
        "                print(f\"File {chain_output_path} only has 1 chain.\")\n",
        "            else:\n",
        "                print(f\"Separated {chain_output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_folder = \"/content/drive/MyDrive/NewPDBFiles\"  # Update this to your folder containing PDB files\n",
        "    output_folder = \"/content/drive/MyDrive/outputPDB\"  # Update this to where you want to save the separate chain files\n",
        "\n",
        "    processed_files = set()  # Maintain a set of processed filenames\n",
        "\n",
        "    for pdb_file in os.scandir(output_folder):\n",
        "        if pdb_file.is_file() and pdb_file.name.startswith(\"4\") and pdb_file.name.endswith(\".pdb\"):\n",
        "            processed_files.add(pdb_file.name[:4])  # Add the first 4 characters to the set\n",
        "\n",
        "    for pdb_file in os.scandir(input_folder):\n",
        "        if pdb_file.is_file() and pdb_file.name.endswith(\".pdb\"):\n",
        "            base_filename = os.path.splitext(pdb_file.name)[0]\n",
        "            if base_filename[:4] in processed_files:\n",
        "                print(f\"Skipping {base_filename} as it's already processed.\")\n",
        "                continue\n",
        "\n",
        "            split_pdb_chains(pdb_file.path, output_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcJqMJEIvtpa"
      },
      "source": [
        "#Part one is DONE!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
